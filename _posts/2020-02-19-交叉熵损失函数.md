# 交叉熵损失函数原理详解  

## 交叉熵简介  

交叉熵是信息论中的一个重要概念，主要用于度量两个概率分布间的差异性，要理解交叉熵，需要先了解下面几个概念。

## 信息量

信息奠基人香农（Shannon）认为“信息是用来消除不确定性的东西”，也就是说衡量信息的大小就是看这个信息消除不确定的程度。  

“太阳从东边升起”，这条信息并没有减少不确定性，因为太阳一定是从东边升起的，这是一句废话。  
“2018年中国队成功进入世界杯”，从直觉上来看，这句话具有很大的信息量。因为中国队进入世界杯的不确定性因素很大，这句话消除了进入世界杯的不确定性，所以按照定义，这句话的信息量很大。  

根据上述可总结如下：**信息量的大小与信息发生的概率成反比**。概率越大，信息量越小；概率越小，信息量越大。  

设某件事情发生的概率为$P(x)$，其信息量表示为：
$$
I(x) = -\log(P(x))
$$  
其中$I(x)$表示为信息量，这里的log表示为以e为底的自然对数。  

## 信息熵

信息熵也被称为熵，用来表示所有信息量的期望。  

所以信息量的熵可以表示为：（这里的$X$是一个离散的随机变量）  
$$  
H(X) = -\sum_{i=1}^n P(x_i)\log(P(x_i)) \qquad (X = x_1,x_2,x_3,...,x_n)
$$  
对于0-1分布的问题，由于其结果只有两种情况，是或者不是，设某一件事情发生的概率为$1-P(x)$，所以对于0-1分布的问题，计算交叉熵的公式可以化简如下：  
$$
H(X) = -\sum_{n=1}^n P(x_i)\log (P(x_i)) \\
=-[P(x)\log(P(x))+(1-P(x))\log (1-P(x))] \\
=-P(x)\log (P(x)) - (1-P(x))\log (1-P(x))
$$  

## 相对熵（KL散度）  

如果对于同一个随机变量$X$有两个单独的概率分布$P(x)和Q(x)$，则我们可以使用KL散度来衡量这两个概率分布之间的差异。  
  
下面直接列出公式，再举例子加以说明。  
$$
D_{KL}(p\|q) = \sum_{i=1}^n p(x_i)\log (\frac{p(x_i)}{q(x_i)})
$$  
再机器学习中，常常使用$P(x)$来表示样本的真实分布，$Q(x)$来表示模型所预测的分布，比如在一个三分类的任务中（例如，猫狗马分类器），$x_1,x_2,x_3$分别代表猫、狗、马，例如一张猫的图片真实分布$P(x)=[1,\ 0,\ 0]$，预测分布$Q(x)=[0.7,\ 0.2,\ 0.1]$，计算KL散度：
$$
D_{KL}(p\|q) = \sum_{i=1}^n p(x_i)\log (\frac{p(x_i)}{q(x_i)}) \\
=p(x_1)\log (\frac{p(x_1)}{q(x_1)})+p(x_2)\log (\frac{p(x_2)}{q(x_2)})+p(x_3)\log (\frac{p(x_3)}{q(x_3)}) \\
=1*\log (\frac{1}{0.7}) = 0.36
$$  
KL散度越小，表示$P(x)$与$Q(x)$的分布更加接近，可以通过反复训练$Q(x)$来使$Q(x)$得分布逼近$P(x)$  

## 交叉熵  

首先将KL散度公式拆开：
$$
D_{KL}(p\|q) = \sum_{i=1}^n p(x_i)\log (\frac{p(x_i)}{q(x_i)})\\
=\sum_{i=1}^n p(x_i)\log (p(x_i)) - \sum_{i=1}^n p(x_i)\log (q(x_i)) \\
=-H(p(x)) + [- \sum_{i=1}^n p(x_i)\log (q(x_i))]
$$  
前者$H(p(x))$表示信息熵，后者即为交叉熵，**KL散度=交叉熵-信息熵**  
交叉熵公式为：
$$
H(p,q) = -\sum_{i=1}^n p(x_i)\log (q(x_i))
$$  
在机器学习训练网络的时候，输入数据与标签常常已经确定，那么真实概率分布$P(x)$也就确定下来了，所以信息熵在这里就是一个常量。由于KL散度的值表示真实概率分布于预测概率分布$Q(x)$之间的差异，值越小表示预测的结果好，所以需要最小化KL散度，而交叉熵等于KL散度加上一个常量（信息熵），且公式相比KL散度更加容易计算，所以在机器学习中常常使用交叉熵损失函数来计算loss就行了。

## 交叉熵在单分类问题中的应用

在线性回归问题中，常常使用MSE(Mean Squared Erro)作为loss函数，而在分类问题中常常使用交叉熵作为作为loss函数。
下面通过一个例子来说明如何计算交叉熵损失值。  

假设我们输入一张狗的图片，标签与预测值如下：  

||猫|狗|马|  
|:-:|:--:|:--:|:--:|  
|Label|0|1|0|  
|Pred|0.2|0.7|0.1|  

那么loss为：  
$$
loss=-(0*\log (0.2) + 1*\log (0.7)+0*\log (0.1) )= 0.36
$$  
一个batch的loss为：  
$$
loss = -\sum_{i=1}^m\sum_{j=1}^n p(x_{ij})\log (q(x_{ij}))
$$
其中m表示样本个数。  

## 总结

1. 交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布于预测概率分布之间的差异。交叉熵的值越小，模型预测的效果就越好。  
2. 交叉熵在分类问题中常常与softmax是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再通过交叉熵来计算损失。  

参考 <https://blog.csdn.net/b1055077005/article/details/100152102>  

## THE END  
